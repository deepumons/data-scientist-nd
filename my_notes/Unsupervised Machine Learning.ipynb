{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unsupervised Machine Learning\n",
    "\n",
    "We use unsupervised machine learning in the following situations:\n",
    "1. Do not have a label to predict. An example of this is using an algorithm to look at brain scans to find areas that may raise concern. You don't have labels on the images to understand what areas might raise reason for concern, but you can understand which areas are most similar or different from one another.\n",
    "\n",
    "2. Are not trying to predict a label, but rather group our data together for some other reason! One example of this is when you have tons of data, and you would like to condense it down to a fewer number of features to be used.\n",
    "\n",
    "There are many methods of unsupervised learning including: clustering, hierarchial and density based clustering, gaussian mixture models and cluster validation, principal component analysis (PCA), and random projection and independenct component analysis. Broadly, unusupervised machine learning can classified into Clustering, and Dimensionality Reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Clustering\n",
    "\n",
    "Clustering algorithms attempts to find groupings of similar items. K-means algorithm is an example.\n",
    "\n",
    "Three ways to identify clusters in your dataset:\n",
    "\n",
    "1. Visual Inspection of your data.\n",
    "2. Pre-conceived ideas of the number of clusters.\n",
    "3. The elbow method, which compares the average distance of each point to the cluster center for different numbers of centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means algorithm\n",
    "\n",
    "K-means is one of the most popular algorithms used for clustering. The 'k' in k-means refers to the number of clusters to form during the execution of the algorithm.\n",
    "\n",
    "It has the following steps:\n",
    "\n",
    "1. Randomly place k-centroids amongst your data. Then repeat steps (2-3) until convergence keeping the number of the centroids the same.\n",
    "2. Look at the distance from each centroid to each point. Assign each point to the closest centroid.\n",
    "3. Move the centroid to the center of the points assigned to it.\n",
    "\n",
    "### Limitations of K-means algorithm\n",
    "\n",
    "There are some concerns with the k-means algorithm:\n",
    "\n",
    "1. Concern: The random placement of the centroids may lead to non-optimal solutions.\n",
    "\n",
    "Solution: Run the algorithm multiple times and choose the centroids that create the smallest average distance of the points to the centroids.\n",
    "\n",
    "2. Concern: Depending on the scale of the features, you may end up with different groupings of your points.\n",
    "\n",
    "Solution: Scale the features using Standardizing, which will create features with mean 0 and standard deviation 1 before running the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Hierarchial clustering\n",
    "\n",
    "### 3.2.1 Single link clustering\n",
    "\n",
    "In single link clustering, the algorithm measures the distance between a point to all other points. It then groups the closest points to a cluster. When a point (that is not part of a cluster) needs to assigned to a cluster, it measures it's distance to the closest point of all clusters. It is then added to the cluster that is closest to it.\n",
    "\n",
    "In the example below, point no 7 is closest to cluster (6, 8) with the closest point 6, and therefore it is assigned to this cluster.\n",
    "\n",
    "![Example for Single link clustering](./images/single_link_clustering_example.png \"Example for Single link clustering\")\n",
    "\n",
    "Single link clustering performs better than k-means in cases where there is a lot of space between the clusters (e.g. case 2 and 3 in the figure below), but performs poorly when the points are too close to each other (e.g. case 1 and 4). It performs just as good as k-means when the the points are natually clustered together (e.g. case 6.)\n",
    "\n",
    "![Single link vs k-means](./images/single_link_vs_k_means.png \"Single link vs k-means\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Complete link clustering\n",
    "\n",
    "Complete link clustering works in a similar fashtion to single link clustering, except that it considers the distance between the two farthest points in a cluster while attempting to merge the two clusters. Complete link clustering produces more compact clusters compared to single link clusters.\n",
    "\n",
    "### 3.2.3 Average link clustering\n",
    "\n",
    "In average link clustering the distance from every point to every other point in the other cluster is measured. The average of these distances are considered before merging the clusters.\n",
    "\n",
    "### 3.2.4 Ward's method\n",
    "\n",
    "Ward's method is the default method for agglomerative clustering in sci-kit learn. This method attempts to minimise the variance while forming clusters.\n",
    "\n",
    "This method first finds the distance between every point in the clusters to the central point between the clusters (yellow X in the figure below). These distances are squared first and added, and from this the vaiance within the clusters are subtracted (distance between the points within a cluster and it's center - red X in the figure below) to arrive at the distance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ward's method](./images/wards_method_example.png \"Ward's method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Density based clustering - DBSCAN\n",
    "\n",
    "DBSCAN stands for Density Based Spatial Clustering of Applications with Noise. Unlike hierarchial clustering methods, DBSCAN does not require the 'number of clusters' as input. It only requires the following paramers as input:\n",
    "\n",
    "1. Epsilon, i.e. the search distance, or radius around a point.\n",
    "2. Minimum number of points required to form a cluster.\n",
    "\n",
    "The algorithm works by visiting each point, and looking for other points with in it's epsilon distance. Once the minimum number of points criterion is satisfied, it forms a cluster. The process is then repeated for other points.\n",
    "\n",
    "DBSCAN has a number of advantages:\n",
    "1. No need to supply the number of clusters as a parameter to the algorithm.\n",
    "2. DBSCAN can effectively deal with a number of cluster shapes and sizes.\n",
    "3. Peforms well in the presence of noise and outliers.\n",
    "\n",
    "Some of the disadvantages:\n",
    "1. Faces difficulties with finding clusters of varying densities.\n",
    "2. Border points that are reachable from two clusters are assigned to a cluster based on first come, first served basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example for DBSCAN](./images/db_scan_example.png \"Example for DBSCAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Gaussian Mixture Models\n",
    "\n",
    "Gaussian Mixture Models is one of the most advanced clustering algorithms. Unlike hierarchial, and density based clustering techniques, a point in the sample in GMM can belong to every cluster with varying membership levels to these clusters.\n",
    "\n",
    "GMM attempts to find gaussian/normal distributions among the sample, one for each value of 'k'.\n",
    "\n",
    "### Expectation Maximization Algorithm\n",
    "\n",
    "1. Initialize K-gaussian distribution\n",
    "\n",
    "This can be done is two ways:\n",
    "a. Assign a random mean and variance to each of the clusters. A naive way of doing this is by assigning the mean and variance of the entire sample to the clusters.\n",
    "b. Perform k-means algorithm to arrive at the initial clusters and there by it's mean and variance.\n",
    "\n",
    "2. Soft cluster the data (Expectation)\n",
    "\n",
    "Use a probablity density function to determine the memberhsip of each point to each cluster.\n",
    "\n",
    "![Expectation Maximization Step 2](./images/gmm_expectation_maximization_step_2.png \"Expectation Maximization Step 2\")\n",
    "\n",
    "3. Re-estimate the gaussian distribution (Maximization)\n",
    "\n",
    "We take the output from step 2 and compute the new mean and variance for the clusters taking the weighted average measures.\n",
    "\n",
    "4. Evaluate the log-likelihood to check for convergence.\n",
    "\n",
    "Evaluate the log-likelihood. Higher the log-likelihood, better the chances that the mixer model generated fits the dataset that we have.\n",
    "\n",
    "5. Repeat steps 2-4 untill convergence is achieved.\n",
    "\n",
    "### Advantages of GMM clustering\n",
    "\n",
    "1. Soft clustering, wherein members of the sample can belong to multiple clusters.\n",
    "2. The algorithm is flexible with shape of the clusters. For example, clusters can even overlap other clusters inside of it.\n",
    "\n",
    "### Disadvantages of GMM clustering\n",
    "\n",
    "1. Highly sensitive to initialization values.\n",
    "2. Convergence rate is slow.\n",
    "3. It's possible to converge to a local optimum.\n",
    "\n",
    "### Applications of GMM clustering\n",
    "\n",
    "1. Categorization of sensor input. For example, classifying accelerometer/GPS tracker data to activities such as walking, biking, running, etc. in fitness trackers.\n",
    "2. Has appliations in astrophysics to classify, stars, pulsars, etc.\n",
    "3. Speaker verification in bio-metrics.\n",
    "4. One of the most well known applications is in computer vision for background/foreground detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Cluster Analysis Process\n",
    "\n",
    "1. Feature Selection\n",
    "\n",
    "Not all features may contribute to effective clustering. It is therefore, important to select the best features for clustering among candidate features.\n",
    "\n",
    "2. Feature Extraction\n",
    "\n",
    "This step encomapsses transforming the data to generate novel and useful features. The Principal Component Analysis (PCA) is an example for this.\n",
    "\n",
    "3. Cluster algorithm selection and tuning\n",
    "\n",
    "Not all algorithms results in the same clustering output. So this step is about selecting the best clustering algorithm that yiels the best results for the given data.\n",
    "\n",
    "4. Cluster validation\n",
    "\n",
    "How do you decide which algorithm yields the best clustering result? Visual observation works with small samples, and small set of features. However, when the feature set is high, we use scoring methods (indices) to arrive at a numeric score for each clustering output.\n",
    "\n",
    "5. Interpretation of the results\n",
    "\n",
    "In the final step explains the clusting result to the stakeholders.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Validation\n",
    "\n",
    "In this step we try to evaluate the clustering results objectively and quantitatively. Cluster validation can be done in three ways:\n",
    "\n",
    "1. External indices:\n",
    "\n",
    "Some times the original labels for the data is avaialble to us. In such cases, we can use this measure.\n",
    "\n",
    "2. Internal indices:\n",
    "\n",
    "In unsupervised learning techniques, we generally do not have access to the labels. In such cases we measure the fit between data and the structure only using data.\n",
    "\n",
    "3. Relative indices:\n",
    "\n",
    "These indicate which of the two clustering structures is better in some sense. All internal indices can serve as relative indices.\n",
    "\n",
    "Most of the indices gives a measure of compactness and separability of clusters. Compactness refers how close the points are together within the clusters. Separability refers how distinct each cluser is among others, i.e. how easy is it separate the clusters, is there enough space between the clusters, etc.\n",
    "\n",
    "#### External validation indices\n",
    "\n",
    "In external indices validation, we match the cluster structure to the information (labels) that we know beforehand to understand how successful was the clustering.\n",
    "\n",
    "In the below figure, we look at the adjusted rand score method which returns a score between the range of -1 to 1.\n",
    "\n",
    "![Cluster validation through external indices](./images/cluster_validation_external_indices.png \"Cluster validation through external indices\")\n",
    "\n",
    "#### Internal validation indices\n",
    "\n",
    "The Silhouette coefficient is a popular internal validation index method. It measures the average distance of a point to other points within the cluster and compares that to the average distance to the points in the closest cluster to arrive at a numeric score between -1 and 1.\n",
    "\n",
    "![Cluster validation through interal indices](./images/cluster_validation_internal_indices.png \"Cluster validation through internal indices\")\n",
    "\n",
    "It has to be noted that silhoutte coefficient works best with dense clusters that are clumped together.\n",
    "\n",
    "![Cluster evaluation through silhouette coefficient 1](./images/silhouette_coefficient_evaluation_1.png \"Cluster evaluation through silhouette coefficient1\")\n",
    "\n",
    "It has to be noted that the silhouette coefficient should never be used with DBSCAN algorithm as it is not designed to handle the concept of noise. It also performs poorly when the data does not form compact clusters, such as the case with the two ring dataset given below\n",
    "\n",
    "![Cluster evaluation through silhouette coefficient 2](./images/silhouette_coefficient_evaluation_2.png \"Cluster evaluation through silhouette coefficient 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
