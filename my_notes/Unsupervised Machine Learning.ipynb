{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unsupervised Machine Learning\n",
    "\n",
    "We use unsupervised machine learning in the following situations:\n",
    "1. Do not have a label to predict. An example of this is using an algorithm to look at brain scans to find areas that may raise concern. You don't have labels on the images to understand what areas might raise reason for concern, but you can understand which areas are most similar or different from one another.\n",
    "\n",
    "2. You are not trying to predict a label, but rather group the data together for some other reason. One example of this is when you have tons of data, and you would like to condense it down to a fewer number of features to be used.\n",
    "\n",
    "There are many methods of unsupervised learning including: clustering, hierarchial and density based clustering, gaussian mixture models and cluster validation, principal component analysis (PCA), and random projection and independenct component analysis. Broadly, unusupervised machine learning can classified into Clustering, and Dimensionality Reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Clustering\n",
    "\n",
    "Clustering algorithms attempts to find groupings of similar items. K-means algorithm is an example.\n",
    "\n",
    "Three ways to identify clusters in your dataset:\n",
    "\n",
    "1. Visual Inspection of your data.\n",
    "2. Pre-conceived ideas of the number of clusters.\n",
    "3. The elbow method, which compares the average distance of each point to the cluster center for different numbers of centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means algorithm\n",
    "\n",
    "K-means is one of the most popular algorithms used for clustering. The 'k' in k-means refers to the number of clusters to form during the execution of the algorithm.\n",
    "\n",
    "It has the following steps:\n",
    "\n",
    "1. Randomly place k-centroids amongst your data.\n",
    "2. Look at the distance from each centroid to each point. Assign each point to the closest centroid.\n",
    "3. Move the centroid to the center of the points assigned to it.\n",
    "4. Then repeat steps (2-3) until convergence keeping the number of the centroids the same.\n",
    "\n",
    "### Limitations of K-means algorithm\n",
    "\n",
    "There are some concerns with the k-means algorithm:\n",
    "\n",
    "1. The random placement of the centroids may lead to non-optimal solutions.\n",
    "\n",
    "Solution: Run the algorithm multiple times and choose the centroids that create the smallest average distance of the points to the centroids.\n",
    "\n",
    "2. Depending on the scale of the features, you may end up with different groupings of your points.\n",
    "\n",
    "Solution: Scale the features using standardizing, which will create features with mean 0 and standard deviation 1 before running the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Hierarchial clustering\n",
    "\n",
    "### 3.2.1 Single link clustering\n",
    "\n",
    "In single link clustering, the algorithm measures the distance between a point to all other points. It then groups the closest points to a cluster. When a point (that is not part of a cluster) needs to assigned to a cluster, it measures it's distance to the closest point of all clusters. It is then added to the cluster that is closest to it.\n",
    "\n",
    "In the example below, point no 7 is closest to cluster (6, 8) with the closest point 6, and therefore it is assigned to this cluster.\n",
    "\n",
    "![Example for Single link clustering](./images/single_link_clustering_example.png \"Example for Single link clustering\")\n",
    "\n",
    "Single link clustering performs better than k-means in cases where there is a lot of space between the clusters (e.g. case 2 and 3 in the figure below), but performs poorly when the points are too close to each other (e.g. case 1 and 4). It performs just as good as k-means when the the points are natually clustered together (e.g. case 6.)\n",
    "\n",
    "![Single link vs k-means](./images/single_link_vs_k_means.png \"Single link vs k-means\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Complete link clustering\n",
    "\n",
    "Complete link clustering works in a similar fashtion to single link clustering, except that it considers the distance between the two farthest points in a cluster while attempting to merge the two clusters. Complete link clustering produces more compact clusters compared to single link clusters.\n",
    "\n",
    "### 3.2.3 Average link clustering\n",
    "\n",
    "In average link clustering, the distance from every point to every other point in the other cluster is measured. The average of these distances are considered before merging the clusters.\n",
    "\n",
    "### 3.2.4 Ward's method\n",
    "\n",
    "Ward's method is the default method for agglomerative clustering in sci-kit learn. This method attempts to minimise the variance while forming clusters.\n",
    "\n",
    "This method first finds the distance between every point in the clusters to the central point between the clusters (yellow X in the figure below). These distances are squared first and added, and from this the vaiance within the clusters are subtracted (distance between the points within a cluster and it's center - red X in the figure below) to arrive at the distance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ward's method](./images/wards_method_example.png \"Ward's method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Density based clustering - DBSCAN\n",
    "\n",
    "DBSCAN stands for Density Based Spatial Clustering of Applications with Noise. Unlike hierarchial clustering methods, DBSCAN does not require the 'number of clusters' as input. It only requires the following paramers as input:\n",
    "\n",
    "1. Epsilon, i.e. the search distance, or radius around a point.\n",
    "2. Minimum number of points required to form a cluster.\n",
    "\n",
    "The algorithm works by visiting each point, and looking for other points with in it's epsilon distance. Once the minimum number of points criterion is satisfied, it forms a cluster. The process is then repeated for other points.\n",
    "\n",
    "DBSCAN has a number of advantages:\n",
    "1. No need to supply the number of clusters as a parameter to the algorithm.\n",
    "2. DBSCAN can effectively deal with a number of cluster shapes and sizes.\n",
    "3. Peforms well in the presence of noise and outliers.\n",
    "\n",
    "Some of the disadvantages:\n",
    "1. Faces difficulties with finding clusters of varying densities.\n",
    "2. Border points that are reachable from two clusters are assigned to a cluster based on first come, first served basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example for DBSCAN](./images/db_scan_example.png \"Example for DBSCAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Gaussian Mixture Models\n",
    "\n",
    "Gaussian Mixture Models is one of the most advanced clustering algorithms. Unlike hierarchial, and density based clustering techniques, a point in the sample in GMM can belong to every cluster with varying membership levels to these clusters.\n",
    "\n",
    "GMM attempts to find gaussian/normal distributions among the sample, one for each value of 'k'.\n",
    "\n",
    "### Expectation Maximization Algorithm\n",
    "\n",
    "1. Initialize K-gaussian distribution\n",
    "\n",
    "This can be done is two ways:\n",
    "a. Assign a random mean and variance to each of the clusters. A naive way of doing this is by assigning the mean and variance of the entire sample to the clusters.\n",
    "b. Perform k-means algorithm to arrive at the initial clusters and there by it's mean and variance.\n",
    "\n",
    "2. Soft cluster the data (Expectation)\n",
    "\n",
    "Use a probablity density function to determine the memberhsip of each point to each cluster.\n",
    "\n",
    "![Expectation Maximization Step 2](./images/gmm_expectation_maximization_step_2.png \"Expectation Maximization Step 2\")\n",
    "\n",
    "3. Re-estimate the gaussian distribution (Maximization)\n",
    "\n",
    "We take the output from step 2 and compute the new mean and variance for the clusters taking the weighted average measures.\n",
    "\n",
    "4. Evaluate the log-likelihood to check for convergence.\n",
    "\n",
    "Evaluate the log-likelihood. Higher the log-likelihood, better the chances that the mixer model generated fits the dataset that we have.\n",
    "\n",
    "5. Repeat steps 2-4 untill convergence is achieved.\n",
    "\n",
    "### Advantages of GMM clustering\n",
    "\n",
    "1. Soft clustering, wherein members of the sample can belong to multiple clusters.\n",
    "2. The algorithm is flexible with shape of the clusters. For example, clusters can even overlap other clusters inside of it.\n",
    "\n",
    "### Disadvantages of GMM clustering\n",
    "\n",
    "1. Highly sensitive to initialization values.\n",
    "2. Convergence rate is slow.\n",
    "3. It's possible to converge to a local optimum.\n",
    "\n",
    "### Applications of GMM clustering\n",
    "\n",
    "1. Categorization of sensor input. For example, classifying accelerometer/GPS tracker data to activities such as walking, biking, running, etc. in fitness trackers.\n",
    "2. Has appliations in astrophysics to classify stars, pulsars, etc.\n",
    "3. Speaker verification in bio-metrics.\n",
    "4. One of the most well known applications is in computer vision for background/foreground detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Cluster Analysis Process\n",
    "\n",
    "1. Feature Selection\n",
    "\n",
    "Not all features may contribute to effective clustering. It is therefore, important to select the best features for clustering among candidate features.\n",
    "\n",
    "2. Feature Extraction\n",
    "\n",
    "This step encomapsses transforming the data to generate novel and useful features. The Principal Component Analysis (PCA) is an example for this.\n",
    "\n",
    "3. Cluster algorithm selection and tuning\n",
    "\n",
    "Not all algorithms results in the same clustering output. So this step is about selecting the best clustering algorithm that yields the best results for the given data.\n",
    "\n",
    "4. Cluster validation\n",
    "\n",
    "How do you decide which algorithm yields the best clustering result? Visual observation works with small samples, and small set of features. However, when the feature set is high, we use scoring methods (indices) to arrive at a numeric score for each clustering output.\n",
    "\n",
    "5. Interpretation of the results\n",
    "\n",
    "The final step explains the clusting result to the stakeholders.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Validation\n",
    "\n",
    "In this step we try to evaluate the clustering results objectively and quantitatively. Cluster validation can be done in three ways:\n",
    "\n",
    "1. External indices:\n",
    "\n",
    "Some times the original labels for the data is avaialble to us. In such cases, we can use this measure.\n",
    "\n",
    "2. Internal indices:\n",
    "\n",
    "In unsupervised learning techniques, we generally do not have access to the labels. In such cases we measure the fit between data and the structure only using data.\n",
    "\n",
    "3. Relative indices:\n",
    "\n",
    "These indicate which of the two clustering structures is better in some sense. All internal indices can serve as relative indices.\n",
    "\n",
    "Most of the indices gives a measure of compactness and separability of clusters. Compactness refers how close the points are together within the clusters. Separability refers how distinct each cluser is among others, i.e. how easy is it separate the clusters, is there enough space between the clusters, etc.\n",
    "\n",
    "#### External validation indices\n",
    "\n",
    "In external indices validation, we match the cluster structure to the information (labels) that we know beforehand to understand how successful was the clustering.\n",
    "\n",
    "In the below figure, we look at the adjusted rand score method which returns a score between the range of -1 to 1.\n",
    "\n",
    "![Cluster validation through external indices](./images/cluster_validation_external_indices.png \"Cluster validation through external indices\")\n",
    "\n",
    "#### Internal validation indices\n",
    "\n",
    "The Silhouette coefficient is a popular internal validation index method. It measures the average distance of a point to other points within the cluster and compares that to the average distance to the points in the closest cluster to arrive at a numeric score between -1 and 1.\n",
    "\n",
    "![Cluster validation through interal indices](./images/cluster_validation_internal_indices.png \"Cluster validation through internal indices\")\n",
    "\n",
    "It has to be noted that silhoutte coefficient works best with dense clusters that are clumped together.\n",
    "\n",
    "![Cluster evaluation through silhouette coefficient 1](./images/silhouette_coefficient_evaluation_1.png \"Cluster evaluation through silhouette coefficient1\")\n",
    "\n",
    "It has to be noted that the silhouette coefficient should never be used with DBSCAN algorithm as it is not designed to handle the concept of noise. It also performs poorly when the data does not form compact clusters, such as the case with the two ring dataset given below\n",
    "\n",
    "![Cluster evaluation through silhouette coefficient 2](./images/silhouette_coefficient_evaluation_2.png \"Cluster evaluation through silhouette coefficient 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction and PCA\n",
    "\n",
    "With large datasets we often suffer with what is known as the \"curse of dimensionality,\" and need to reduce the number of features to effectively develop a model. Feature Selection and Feature Extraction are two general approaches for reducing dimensionality.\n",
    "\n",
    "#### Feature Selection\n",
    "Feature Selection involves finding a subset of the original features of your data that you determine are most relevant and useful. Feature selection is of two kinds:\n",
    "\n",
    "1. **Filter methods** Filtering approaches use a ranking or sorting algorithm to filter out those features that have less usefulness.Filter methods are based on discerning some inherent correlations among the feature data in unsupervised learning, or on correlations with the output variable in supervised settings. Filter methods are usually applied as a preprocessing step. Common tools for determining correlations in filter methods include: Pearson's Correlation, Linear Discriminant Analysis (LDA), and Analysis of Variance (ANOVA).\n",
    "\n",
    "2. **Wrapper methods** Wrapper approaches generally select features by directly testing their impact on the performance of a model. The idea is to \"wrap\" this procedure around your algorithm, repeatedly calling the algorithm using different subsets of features, and measuring the performance of each model. Cross-validation is used across these multiple tests. The features that produce the best models are selected. Clearly this is a computationally expensive approach for finding the best performing subset of features, since they have to make a number of calls to the learning algorithm. Common examples of wrapper methods are: Forward Search, Backward Search, and Recursive Feature Elimination.\n",
    "\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "**Principal Component Analysis** is a common method for extracting, or constructing new \"latent features\" from our dataset, based on existing features. Other methods for accomplishing Feature Extraction include **Independent Component Analysis (ICA)** and **Random Projection**.\n",
    "\n",
    "An advantage of Feature Extraction over Feature Selection is that the latent features can be constructed to incorporate data from multiple features, and thus retain more information present in the various original inputs, than just losing that information by dropping many original inputs.\n",
    "\n",
    "#### Principal Components\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique that is used to reduce the dimensionality of your dataset. The reduced features are called principal components, or latent features. These principal components are simply a linear combination of the original features in your dataset.\n",
    "\n",
    "There are two main properties of principal components:\n",
    "\n",
    "1. They retain the most amount of information in the dataset. In the image below, we can see that retaining the most information in the dataset means finding a line that reduced the distances of the points to the component across all the points (same as in regression), i.e. the line that has the least information loss.\n",
    "\n",
    "2. The created components are orthogonal to (independent of) one another. So far we have been mostly focused on what the first component of a dataset would look like. However, when there are many components, the additional components will all be orthogonal to one another. Depending on how the components are used, there are benefits to having orthogonal components. In regression, we often would like independent features, so using the components in regression now guarantees this.\n",
    "\n",
    "![PCA criterion](./images/pca_criterion.png \"PCA criterion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "\n",
    "There are two major parts to interpreting the PCA results:\n",
    "\n",
    "1. The variance explained by each component. We can visualize this with scree plots to understand how many components you might keep based on how much information was being retained.\n",
    "2. The principal components themselves, which gave us an idea of which original features were most related to why a component was able to explain certain aspects about the original datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
