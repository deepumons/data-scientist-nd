{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "One can say Statistics + Computer Science = Machine Learning. The field of machine learning arises from the developments in Comupter Scince and application of Statistics.\n",
    "\n",
    "The statistical interpretation of Machine Learning comes from understanding data. Statistics has developed probablity models (Poisson distribution, bell curves, etc.) to describe describe the world around us based on the input data. However, when we generate data based on these models, they often do not line up with real world.\n",
    "\n",
    "Scientits realised that it's better to develop models to correctly predict where the data should go, instead of writing endless if-else statements, i.e. create and train models to recognise patterns among the given data.\n",
    "\n",
    "Initially models based on popular statistical distributions were used to generate data. However, this introduces discrepencies between the generated data and the real world data. Machine Learning, these days, uses flexible models rather than following standard statistical models. This imporoves performance and accuracy, but introduces a new problem - It may not be possible to identify why a particular decision was taken by the model.\n",
    "\n",
    "#### Types of Machine Learning\n",
    "\n",
    "ML can be boadly classified into three types: Supervised, Unsupervised, and Reinforcement Learning.\n",
    "\n",
    "1) Supervised Learning\n",
    "\n",
    "These models looks at the already available data and it's labels (or categories) to train the models and predict the category of the new data item. Supervised Learning can be further classified into Classification and Regression.\n",
    "\n",
    "a) Classification methods decides if the data belongs to either or one or the other category, or one among many categories. For example, deciding if the new mail received is spam or not is an example for classification method. So is deciding on the breed of a dog among many dog breeds.\n",
    "\n",
    "b) Regression methods are meant for predicting numeric values. Predicting housing values, or height of a person, etc. are examples for this.\n",
    "\n",
    "2) Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is used to create models in the absense of any pre exiting labels, or data to train on. Grouping data, building recommendation engines, etc. are good examples for this.\n",
    "\n",
    "3) Reinforcement Learning\n",
    "\n",
    "In this method, the model performs an action and learns from it's outcome, i.e. weather it was positive, or negative, good or bad, etc. Thus the outcome further reinforces the models understanding of the data, and it takes the lessons learnt into account going into the next iteration.\n",
    "\n",
    "It's to be noted that Reinforcement Learning methods do not have many applications in Data Science as bulk of the Data Science problems fall into Supervised or Unsupervised learning.\n",
    "\n",
    "#### Deep Learning\n",
    "\n",
    "Deep Learning can be seen as Machine Learning on Steroids. Deep Learning can be used for all three machine learning types discussed earlier, but it's application may not be useful in all situations. Deep learning requires a tremendous amount of data, and computing power to generate results compared with other machine learning methods.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "Bayes Theorem is one of the earliest probabilistic inference algorithms. It was developed by Reverend Bayes (which he used to try and infer the existence of God no less), and still performs extremely well for certain use cases. Bayes theorem calculates the probability of an event occurring, based on certain other probabilities that are related to the event in question. It is composed of a prior(the probabilities that we are aware of or that is given to us) and the posterior(the probabilities we are looking to compute using the priors).\n",
    "\n",
    "Let's say we are trying to find the odds of an individual having diabetes, given that he or she was tested for it and got a positive result. In the medical field, such probabilies play a very important role as it usually deals with life and death situations.\n",
    "\n",
    "We assume the following:\n",
    "\n",
    "P(D) is the probability of a person having Diabetes. It's value is 0.01 or in other words, 1% of the general population has diabetes(Disclaimer: these values are assumptions and are not reflective of any medical study).\n",
    "\n",
    "P(Pos) is the probability of getting a positive test result.\n",
    "\n",
    "P(Neg) is the probability of getting a negative test result.\n",
    "\n",
    "P(Pos|D) is the probability of getting a positive result on a test done for detecting diabetes, given that you have diabetes. This has a value 0.9. In other words the test is correct 90% of the time. This is also called the Sensitivity or True Positive Rate.\n",
    "\n",
    "P(Neg|~D) is the probability of getting a negative result on a test done for detecting diabetes, given that you do not have diabetes. This also has a value of 0.9 and is therefore correct, 90% of the time. This is also called the Specificity or True Negative Rate.\n",
    "\n",
    "The Bayes formula is as follows:\n",
    "\n",
    "$$ P(A|B) = P(B|A) * P(A) / P(B) $$\n",
    "\n",
    "P(A) is the prior probability of A occurring independently. In our example this is P(D). This value is given to us.\n",
    "\n",
    "P(B) is the prior probability of B occurring independently. In our example this is P(Pos).\n",
    "\n",
    "P(A|B) is the posterior probability that A occurs given B. In our example this is P(D|Pos). That is, the probability of an individual having diabetes, given that, that individual got a positive test result. This is the value that we are looking to calculate.\n",
    "\n",
    "P(B|A) is the likelihood probability of B occurring, given A. In our example this is P(Pos|D). This value is given to us.\n",
    "\n",
    "Putting our values into the formula for Bayes theorem we get:\n",
    "\n",
    "$$ P(D|Pos) = P(D) * P(Pos|D) / P(Pos) $$\n",
    "\n",
    "The probability of getting a positive test result P(Pos) can be calculated using the Sensitivity and Specificity as follows:\n",
    "\n",
    "$$ P(Pos) = [P(D) * Sensitivity] + [P(D') * (1-Specificity)] $$\n",
    "\n",
    "Using all this we can understand that the probability of an individual having diabetes, given that, that individual got a positive test result:\n",
    "\n",
    "$$ P(D|Pos) = P(D) * Sensitivity) / P(Pos) $$\n",
    "\n",
    "The probability of an individual not having diabetes, given that, that individual got a positive test result:\n",
    "\n",
    "$$ P(D'|Pos) = P(D') * (1-Specificity) / P(Pos) $$\n",
    "\n",
    "The sum of our posteriors will always equal 1.\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "The term 'Naive' in Naive Bayes comes from the fact that the algorithm considers the features that it is using to make the predictions to be independent of each other, which may not always be the case. Naive Bayes is an extension of Bayes' theorem that assumes that all the features are independent of each other, and we can consider cases where we have more than one feature.\n",
    "\n",
    "Let's say that we have two political parties' candidates, 'Jill Stein' of the Green Party and 'Gary Johnson' of the Libertarian Party and we have the probabilities of each of these candidates saying the words 'freedom', 'immigration' and 'environment' when they give a speech:\n",
    "\n",
    "* Probability that Jill Stein says 'freedom': 0.1 ---------> P(F|J)\n",
    "* Probability that Jill Stein says 'immigration': 0.1 -----> P(I|J)\n",
    "* Probability that Jill Stein says 'environment': 0.8 -----> P(E|J)\n",
    "* Probability that Gary Johnson says 'freedom': 0.7 -------> P(F|G)\n",
    "* Probability that Gary Johnson says 'immigration': 0.2 ---> P(I|G)\n",
    "* Probability that Gary Johnson says 'environment': 0.1 ---> P(E|G)\n",
    "* And let us also assume that the probability of Jill Stein giving a speech, P(J) is 0.5 and the same for Gary Johnson, P(G) = 0.5.\n",
    "\n",
    "Given this, what if we had to find the probabilities of Jill Stein saying the words 'freedom' and 'immigration'? This is where the Naive Bayes' theorem comes into play as we are considering two features, 'freedom' and 'immigration'.\n",
    "\n",
    "Now we are at a place where we can define the formula for the Naive Bayes' theorem:\n",
    "\n",
    "Here, y is the class variable (in our case the name of the candidate) and x1 through xn are the feature vectors (in our case the individual words). The theorem makes the assumption that each of the feature vectors or words (xi) are independent of each other.\n",
    "\n",
    "To break this down, we have to compute the following posterior probabilities:\n",
    "\n",
    "P(J|F,I): Given the words freedom and immigration were said, what's the probability that it was said by Jill?\n",
    "\n",
    "Using the formula and our knowledge of Bayes' theorem, we can compute this as follows: P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I). Here P(F,I) is the probability of the words 'freedom' and 'immigration' being said in a speech.\n",
    "\n",
    "P(G|F,I): Probability that words Freedom and Immigration are said by Gary Johnson.\n",
    "\n",
    "Using the formula, we can compute this as follows: P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I)\n",
    "\n",
    "Here, P(F,I)is the probablity of the words Freedom and Immigration being said = \\[(P(J) * P(F|J) * P(I|J)\\] + \\[(P(G) * P(F|G) * P(I|G)\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words freedom and immigration being said are:  0.075\n",
      "The probability of Jill Stein saying the words Freedom and Immigration:  0.06666666666666668\n",
      "The probability of Gary Johnson saying the words Freedom and Immigration:  0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instructions: Compute the probability of the words 'freedom' and 'immigration' being said in a speech, or\n",
    "P(F,I).\n",
    "\n",
    "The first step is multiplying the probabilities of Jill Stein giving a speech with her individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_j_text.\n",
    "\n",
    "The second step is multiplying the probabilities of Gary Johnson giving a speech with his individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_g_text.\n",
    "\n",
    "The third step is to add both of these probabilities and you will get P(F,I).\n",
    "'''\n",
    "\n",
    "'''\n",
    "Solution: Step 1\n",
    "'''\n",
    "# P(J)\n",
    "p_j = 0.5\n",
    "\n",
    "# P(F/J)\n",
    "p_j_f = 0.1\n",
    "\n",
    "# P(I/J)\n",
    "p_j_i = 0.1\n",
    "\n",
    "p_j_text = p_j * p_j_f * p_j_i\n",
    "#print(p_j_text)\n",
    "\n",
    "'''\n",
    "Solution: Step 2\n",
    "'''\n",
    "# P(G)\n",
    "p_g = 0.5\n",
    "\n",
    "# P(F/G)\n",
    "p_g_f = 0.7\n",
    "\n",
    "# P(I/G)\n",
    "p_g_i = 0.2\n",
    "\n",
    "p_g_text = p_g * p_g_f * p_g_i\n",
    "#print(p_g_text)\n",
    "\n",
    "'''\n",
    "Solution: Step 3: Compute P(F,I) and store in p_f_i\n",
    "'''\n",
    "p_f_i = p_j_text + p_g_text\n",
    "print('Probability of words freedom and immigration being said are: ', format(p_f_i))\n",
    "\n",
    "'''\n",
    "Instructions:\n",
    "Compute P(J|F,I) using the formula P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I) and store it in a variable p_j_fi\n",
    "'''\n",
    "\n",
    "'''\n",
    "Solution\n",
    "'''\n",
    "p_j_fi = (p_j * p_j_f * p_j_i ) / p_f_i\n",
    "print('The probability of Jill Stein saying the words Freedom and Immigration: ', format(p_j_fi))\n",
    "\n",
    "'''\n",
    "Instructions:\n",
    "Compute P(G|F,I) using the formula P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I) and store it in a variable p_g_fi\n",
    "'''\n",
    "\n",
    "'''\n",
    "Solution\n",
    "'''\n",
    "p_g_fi = (p_g * p_g_f * p_g_i) / p_f_i\n",
    "print('The probability of Gary Johnson saying the words Freedom and Immigration: ', format(p_g_fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we can see, just like in the Bayes' theorem case, the sum of our posteriors is equal to 1. Our analysis shows that there is only a 6.6% chance that Jill Stein of the Green Party uses the words 'freedom' and 'immigration' in her speech as compared the the 93.3% chance for Gary Johnson of the Libertarian party."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major advantages that Naive Bayes has over other classification algorithms is its ability to handle an extremely large number of features. In our case, each word is treated as a feature and there are thousands of different words. Also, it performs well even with the presence of irrelevant features and is relatively unaffected by them. The other major advantage it has is its relative simplicity. Naive Bayes' works well right out of the box and tuning it's parameters is rarely ever necessary, except usually in cases where the distribution of the data is known. It rarely ever overfits the data. Another important advantage is that its model training and prediction times are very fast for the amount of data it can handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "SVMs are a popular algorithm used for classification problems. SVMs not only classify the data, but also aims to find the best possible boundaries that maintains the largest possible distance between the different classes.\n",
    "\n",
    "The three different ways that SVMs can be implemented are:\n",
    "\n",
    "1. Maximum Margin Classifier\n",
    "2. Classification with Inseparable Classes\n",
    "3. Kernel Methods\n",
    "\n",
    "#### Maximum Margin Classifier\n",
    "\n",
    "When your data can be completely separated, the linear version of SVMs attempts to maximize the distance from the linear boundary to the closest points (called the support vectors). For this reason, we can see that in the picture below, the boundary on the left is better than the one on the right.\n",
    "\n",
    "![Example for Maximum margin classifer](./images/svm_maximum_margin_classifier.png \"Example for Maximum margin classifer\")\n",
    "\n",
    "#### Classification with Inseparable Classes\n",
    "\n",
    "Unfortunately, data in the real world is rarely completely separable as shown in the above image. For this reason, we can make use of a new hyper-parameter called C. The C hyper-parameter determines how flexible we are willing to be with the points that fall on the wrong side of our dividing boundary. The value of C ranges between 0 and infinity. When C is large, you are forcing your boundary to have fewer errors than when it is a small value. The margin may also reduce with a larger value for C.\n",
    "\n",
    "![Example for classification with inseparable classes](./images/svm_classification_with_inseparable_classes.png \"Example for classification with inseparable classes\")\n",
    "\n",
    "#### Kernels\n",
    "\n",
    "Finally, we can look at what makes SVMs truly powerful, kernels. Kernels in SVMs allow us the ability to separate data when the boundary between them is nonlinear. For example let's look at the follwing types of kernels:\n",
    "\n",
    "* Polynomial\n",
    "* RBF\n",
    "\n",
    "Both the methods work by projecting the points to a higher plane, or dimension, where the points can be easily separated using a kernel function. Once this is done, the same can be projected to the (initial) lower plane or dimension.\n",
    "\n",
    "By far the most popular kernel is the rbf kernel (which stands for radial basis function). The rbf kernel allows you the opportunity to classify points that seem hard to separate in any space. This is a density based approach that looks at the closeness of points to one another. This introduces another hyper-parameter gamma. When gamma is large, the outcome is similar to having a large value of C, that is your algorithm will attempt to classify every point correctly. Alternatively, small values of gamma will try to cluster in a more general way that will make more mistakes, but may perform better when it sees new data.\n",
    "\n",
    "![](./images/svm_kernels.png) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
