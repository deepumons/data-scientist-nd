{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "One can say Statistics + Computer Science = Machine Learning. The field of machine learning arises from the developments in Comupter Scince and application of Statistics.\n",
    "\n",
    "The statistical interpretation of Machine Learning comes from understanding data. Statistics has developed probablity models (Poisson distribution, bell curves, etc.) to describe describe the world around us based on the input data. However, when we generate data based on these models, they often do not line up with real world.\n",
    "\n",
    "Scientits realised that it's better to develop models to correctly predict where the data should go, instead of writing endless if-else statements, i.e. create and train models to recognise patterns among the given data.\n",
    "\n",
    "Initially models based on popular statistical distributions were used to generate data. However, this introduces discrepencies between the generated data and the real world data. Machine Learning, these days, uses flexible models rather than following standard statistical models. This imporoves performance and accuracy, but introduces a new problem - It may not be possible to identify why a particular decision was taken by the model.\n",
    "\n",
    "#### Types of Machine Learning\n",
    "\n",
    "ML can be boadly classified into three types: Supervised, Unsupervised, and Reinforcement Learning.\n",
    "\n",
    "1) Supervised Learning\n",
    "\n",
    "These models looks at the already available data and it's labels (or categories) to train the models and predict the category of the new data item. Supervised Learning can be further classified into Classification and Regression.\n",
    "\n",
    "a) Classification methods decides if the data belongs to either or one or the other category, or one among many categories. For example, deciding if the new mail received is spam or not is an example for classification method. So is deciding on the breed of a dog among many dog breeds.\n",
    "\n",
    "b) Regression methods are meant for predicting numeric values. Predicting housing values, or height of a person, etc. are examples for this.\n",
    "\n",
    "2) Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is used to create models in the absense of any pre exiting labels, or data to train on. Grouping data, building recommendation engines, etc. are good examples for this.\n",
    "\n",
    "3) Reinforcement Learning\n",
    "\n",
    "In this method, the model performs an action and learns from it's outcome, i.e. weather it was positive, or negative, good or bad, etc. Thus the outcome further reinforces the models understanding of the data, and it takes the lessons learnt into account going into the next iteration.\n",
    "\n",
    "It's to be noted that Reinforcement Learning methods do not have many applications in Data Science as bulk of the Data Science problems fall into Supervised or Unsupervised learning.\n",
    "\n",
    "#### Deep Learning\n",
    "\n",
    "Deep Learning can be seen as Machine Learning on Steroids. Deep Learning can be used for all three machine learning types discussed earlier, but it's application may not be useful in all situations. Deep learning requires a tremendous amount of data, and computing power to generate results compared with other machine learning methods.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "Bayes Theorem is one of the earliest probabilistic inference algorithms. It was developed by Reverend Bayes (which he used to try and infer the existence of God no less), and still performs extremely well for certain use cases. Bayes theorem calculates the probability of an event occurring, based on certain other probabilities that are related to the event in question. It is composed of a prior(the probabilities that we are aware of or that is given to us) and the posterior(the probabilities we are looking to compute using the priors).\n",
    "\n",
    "Let's say we are trying to find the odds of an individual having diabetes, given that he or she was tested for it and got a positive result. In the medical field, such probabilies play a very important role as it usually deals with life and death situations.\n",
    "\n",
    "We assume the following:\n",
    "\n",
    "P(D) is the probability of a person having Diabetes. It's value is 0.01 or in other words, 1% of the general population has diabetes(Disclaimer: these values are assumptions and are not reflective of any medical study).\n",
    "\n",
    "P(Pos) is the probability of getting a positive test result.\n",
    "\n",
    "P(Neg) is the probability of getting a negative test result.\n",
    "\n",
    "P(Pos|D) is the probability of getting a positive result on a test done for detecting diabetes, given that you have diabetes. This has a value 0.9. In other words the test is correct 90% of the time. This is also called the Sensitivity or True Positive Rate.\n",
    "\n",
    "P(Neg|~D) is the probability of getting a negative result on a test done for detecting diabetes, given that you do not have diabetes. This also has a value of 0.9 and is therefore correct, 90% of the time. This is also called the Specificity or True Negative Rate.\n",
    "\n",
    "The Bayes formula is as follows:\n",
    "\n",
    "$$ P(A|B) = P(B|A) * P(A) / P(B) $$\n",
    "\n",
    "P(A) is the prior probability of A occurring independently. In our example this is P(D). This value is given to us.\n",
    "\n",
    "P(B) is the prior probability of B occurring independently. In our example this is P(Pos).\n",
    "\n",
    "P(A|B) is the posterior probability that A occurs given B. In our example this is P(D|Pos). That is, the probability of an individual having diabetes, given that, that individual got a positive test result. This is the value that we are looking to calculate.\n",
    "\n",
    "P(B|A) is the likelihood probability of B occurring, given A. In our example this is P(Pos|D). This value is given to us.\n",
    "\n",
    "Putting our values into the formula for Bayes theorem we get:\n",
    "\n",
    "$$ P(D|Pos) = P(D) * P(Pos|D) / P(Pos) $$\n",
    "\n",
    "The probability of getting a positive test result P(Pos) can be calculated using the Sensitivity and Specificity as follows:\n",
    "\n",
    "$$ P(Pos) = [P(D) * Sensitivity] + [P(D') * (1-Specificity)] $$\n",
    "\n",
    "Using all this we can understand that the probability of an individual having diabetes, given that, that individual got a positive test result:\n",
    "\n",
    "$$ P(D|Pos) = P(D) * Sensitivity) / P(Pos) $$\n",
    "\n",
    "The probability of an individual not having diabetes, given that, that individual got a positive test result:\n",
    "\n",
    "$$ P(D'|Pos) = P(D') * (1-Specificity) / P(Pos) $$\n",
    "\n",
    "The sum of our posteriors will always equal 1.\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "The term 'Naive' in Naive Bayes comes from the fact that the algorithm considers the features that it is using to make the predictions to be independent of each other, which may not always be the case. Naive Bayes is an extension of Bayes' theorem that assumes that all the features are independent of each other, and we can consider cases where we have more than one feature.\n",
    "\n",
    "Let's say that we have two political parties' candidates, 'Jill Stein' of the Green Party and 'Gary Johnson' of the Libertarian Party and we have the probabilities of each of these candidates saying the words 'freedom', 'immigration' and 'environment' when they give a speech:\n",
    "\n",
    "* Probability that Jill Stein says 'freedom': 0.1 ---------> P(F|J)\n",
    "* Probability that Jill Stein says 'immigration': 0.1 -----> P(I|J)\n",
    "* Probability that Jill Stein says 'environment': 0.8 -----> P(E|J)\n",
    "* Probability that Gary Johnson says 'freedom': 0.7 -------> P(F|G)\n",
    "* Probability that Gary Johnson says 'immigration': 0.2 ---> P(I|G)\n",
    "* Probability that Gary Johnson says 'environment': 0.1 ---> P(E|G)\n",
    "* And let us also assume that the probability of Jill Stein giving a speech, P(J) is 0.5 and the same for Gary Johnson, P(G) = 0.5.\n",
    "\n",
    "Given this, what if we had to find the probabilities of Jill Stein saying the words 'freedom' and 'immigration'? This is where the Naive Bayes' theorem comes into play as we are considering two features, 'freedom' and 'immigration'.\n",
    "\n",
    "Now we are at a place where we can define the formula for the Naive Bayes' theorem:\n",
    "\n",
    "Here, y is the class variable (in our case the name of the candidate) and x1 through xn are the feature vectors (in our case the individual words). The theorem makes the assumption that each of the feature vectors or words (xi) are independent of each other.\n",
    "\n",
    "To break this down, we have to compute the following posterior probabilities:\n",
    "\n",
    "P(J|F,I): Given the words freedom and immigration were said, what's the probability that it was said by Jill?\n",
    "\n",
    "Using the formula and our knowledge of Bayes' theorem, we can compute this as follows: P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I). Here P(F,I) is the probability of the words 'freedom' and 'immigration' being said in a speech.\n",
    "\n",
    "P(G|F,I): Probability that words Freedom and Immigration are said by Gary Johnson.\n",
    "\n",
    "Using the formula, we can compute this as follows: P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I)\n",
    "\n",
    "Here, P(F,I)is the probablity of the words Freedom and Immigration being said = \\[(P(J) * P(F|J) * P(I|J)\\] + \\[(P(G) * P(F|G) * P(I|G)\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words freedom and immigration being said are:  0.075\n",
      "The probability of Jill Stein saying the words Freedom and Immigration:  0.06666666666666668\n",
      "The probability of Gary Johnson saying the words Freedom and Immigration:  0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instructions: Compute the probability of the words 'freedom' and 'immigration' being said in a speech, or\n",
    "P(F,I).\n",
    "\n",
    "The first step is multiplying the probabilities of Jill Stein giving a speech with her individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_j_text.\n",
    "\n",
    "The second step is multiplying the probabilities of Gary Johnson giving a speech with his individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_g_text.\n",
    "\n",
    "The third step is to add both of these probabilities and you will get P(F,I).\n",
    "'''\n",
    "\n",
    "'''\n",
    "Solution: Step 1\n",
    "'''\n",
    "# P(J)\n",
    "p_j = 0.5\n",
    "\n",
    "# P(F/J)\n",
    "p_j_f = 0.1\n",
    "\n",
    "# P(I/J)\n",
    "p_j_i = 0.1\n",
    "\n",
    "p_j_text = p_j * p_j_f * p_j_i\n",
    "#print(p_j_text)\n",
    "\n",
    "'''\n",
    "Solution: Step 2\n",
    "'''\n",
    "# P(G)\n",
    "p_g = 0.5\n",
    "\n",
    "# P(F/G)\n",
    "p_g_f = 0.7\n",
    "\n",
    "# P(I/G)\n",
    "p_g_i = 0.2\n",
    "\n",
    "p_g_text = p_g * p_g_f * p_g_i\n",
    "#print(p_g_text)\n",
    "\n",
    "'''\n",
    "Solution: Step 3: Compute P(F,I) and store in p_f_i\n",
    "'''\n",
    "p_f_i = p_j_text + p_g_text\n",
    "print('Probability of words freedom and immigration being said are: ', format(p_f_i))\n",
    "\n",
    "'''\n",
    "Instructions:\n",
    "Compute P(J|F,I) using the formula P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I) and store it in a variable p_j_fi\n",
    "'''\n",
    "\n",
    "'''\n",
    "Solution\n",
    "'''\n",
    "p_j_fi = (p_j * p_j_f * p_j_i ) / p_f_i\n",
    "print('The probability of Jill Stein saying the words Freedom and Immigration: ', format(p_j_fi))\n",
    "\n",
    "'''\n",
    "Instructions:\n",
    "Compute P(G|F,I) using the formula P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I) and store it in a variable p_g_fi\n",
    "'''\n",
    "\n",
    "'''\n",
    "Solution\n",
    "'''\n",
    "p_g_fi = (p_g * p_g_f * p_g_i) / p_f_i\n",
    "print('The probability of Gary Johnson saying the words Freedom and Immigration: ', format(p_g_fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we can see, just like in the Bayes' theorem case, the sum of our posteriors is equal to 1. Our analysis shows that there is only a 6.6% chance that Jill Stein of the Green Party uses the words 'freedom' and 'immigration' in her speech as compared the the 93.3% chance for Gary Johnson of the Libertarian party."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major advantages that Naive Bayes has over other classification algorithms is its ability to handle an extremely large number of features. In our case, each word is treated as a feature and there are thousands of different words. Also, it performs well even with the presence of irrelevant features and is relatively unaffected by them. The other major advantage it has is its relative simplicity. Naive Bayes' works well right out of the box and tuning it's parameters is rarely ever necessary, except usually in cases where the distribution of the data is known. It rarely ever overfits the data. Another important advantage is that its model training and prediction times are very fast for the amount of data it can handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "SVMs are a popular algorithm used for classification problems. SVMs not only classify the data, but also aims to find the best possible boundaries that maintains the largest possible distance between the different classes.\n",
    "\n",
    "The three different ways that SVMs can be implemented are:\n",
    "\n",
    "1. Maximum Margin Classifier\n",
    "2. Classification with Inseparable Classes\n",
    "3. Kernel Methods\n",
    "\n",
    "#### Maximum Margin Classifier\n",
    "\n",
    "When your data can be completely separated, the linear version of SVMs attempts to maximize the distance from the linear boundary to the closest points (called the support vectors). For this reason, we can see that in the picture below, the boundary on the left is better than the one on the right.\n",
    "\n",
    "![Example for Maximum margin classifer](./images/svm_maximum_margin_classifier.png \"Example for Maximum margin classifer\")\n",
    "\n",
    "#### Classification with Inseparable Classes\n",
    "\n",
    "Unfortunately, data in the real world is rarely completely separable as shown in the above image. For this reason, we can make use of a new hyper-parameter called C. The C hyper-parameter determines how flexible we are willing to be with the points that fall on the wrong side of our dividing boundary. The value of C ranges between 0 and infinity. When C is large, you are forcing your boundary to have fewer errors than when it is a small value. The margin may also reduce with a larger value for C.\n",
    "\n",
    "![Example for classification with inseparable classes](./images/svm_classification_with_inseparable_classes.png \"Example for classification with inseparable classes\")\n",
    "\n",
    "#### Kernels\n",
    "\n",
    "Finally, we can look at what makes SVMs truly powerful, kernels. Kernels in SVMs allow us the ability to separate data when the boundary between them is nonlinear. For example let's look at the follwing types of kernels:\n",
    "\n",
    "* Polynomial\n",
    "* RBF\n",
    "\n",
    "Both the methods work by projecting the points to a higher plane, or dimension, where the points can be easily separated using a kernel function. Once this is done, the same can be projected to the (initial) lower plane or dimension.\n",
    "\n",
    "By far the most popular kernel is the rbf kernel (which stands for radial basis function). The rbf kernel allows you the opportunity to classify points that seem hard to separate in any space. This is a density based approach that looks at the closeness of points to one another. This introduces another hyper-parameter gamma. When gamma is large, the outcome is similar to having a large value of C, that is your algorithm will attempt to classify every point correctly. Alternatively, small values of gamma will try to cluster in a more general way that will make more mistakes, but may perform better when it sees new data.\n",
    "\n",
    "![](./images/svm_kernels.png) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "Ensemble methods allows us to combine (or ensemble) the models you have already seen in a way that makes the combination of these models better at predicting than the individual models.\n",
    "\n",
    "#### Why Would We Want to Ensemble Learners Together?\n",
    "\n",
    "There are two competing variables in finding a well fitting machine learning model: Bias and Variance.\n",
    "\n",
    "Bias: When a model has high bias, this means that it doesn't do a good job of bending to the data (in flexible). An example of an algorithm that usually has high bias is linear regression. Even with completely different datasets, we end up with the same line fit to the data. When models have high bias, this is bad.\n",
    "\n",
    "![Example for linear regression bias](./images/linear_regression_bias.png \"Example for linear regression bias\")\n",
    "\n",
    "Variance: When a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset. Linear models like the one above is low variance, but high bias. An example of an algorithm that tends to have a high variance and low bias is a decision tree (especially decision trees with no early stopping parameters). A decision tree, as a high variance algorithm, will attempt to split every point into it's own branch if possible. They are extremely flexible but may tend to to fit exactly whatever data they see.\n",
    "\n",
    "By combining algorithms, we can often build models that perform better by meeting in the middle in terms of bias and variance.\n",
    "\n",
    "#### Introducing Randomness Into Ensembles\n",
    "\n",
    "Another method that is used to improve ensemble methods is to introduce randomness into high variance algorithms before they are ensembled together. The introduction of randomness combats the tendency of these algorithms to overfit (or fit directly to the data available). There are two main ways that randomness is introduced:\n",
    "\n",
    "Bootstrap the data - that is, sampling the data with replacement and fitting your algorithm and fitting your algorithm to the sampled data.\n",
    "\n",
    "Subset the features - in each split of a decision tree or with each algorithm used an ensemble only a subset of the total possible features are used.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics\n",
    "\n",
    "Models that \"best-fit\" the training data may not always give best results. This could be due to \"over fitting\", wherein the model might have memorised the data, and such models may not generalise well with the real world data. So how do we knnow how well the model is doing? We can do so by testing the model!\n",
    "\n",
    "## Training & Testing Data\n",
    "\n",
    "It is important to always split your data into training and testing. Then you can measure how well your model performs on the test set of data after being fit training data.\n",
    "\n",
    "## Classification Metrics\n",
    "\n",
    "If you are fitting your model to predict categorical data (spam not spam), there are different measures to understand how well your model is performing than if you are predicting numeric values (the price of a home).\n",
    "\n",
    "In pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Both precision and recall are therefore based on an understanding and measure of relevance.\n",
    "\n",
    "Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12. When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3. So, in this case, precision is \"how useful the search results are\", and recall is \"how complete the results are\".\n",
    "\n",
    "In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results. The relationship  between precision, and recall is illustrated in the \"confusion matrix\" below:\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "![Diagram for confusion matrix](./images/confusion_matrix.png \"Diagram for confusion matrix\")\n",
    "\n",
    "#### Precision\n",
    "\n",
    "From the confusion matrix above, $$ Precision = \\frac{True Positives}{True Positives + False Positives}$$\n",
    "\n",
    "i.e., precision is the ratio of relevant results to the predicted positive resutls.\n",
    "\n",
    "#### Recall\n",
    "\n",
    "$$ Recall = \\frac{True Positives} {True Positives + False Negatives} $$\n",
    "\n",
    "i.e., recall is ratio of the relevant results to the actual positive results. Recall is also called as True Positive Rate (TPR).\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "$$ Accuracy = \\frac{True Positives + True Negatives} { True Positives + True Negatives + False Positives + False Negatives} $$\n",
    "\n",
    "i.e., Accuracy is the ratio of observations we correctly labeled among all results.\n",
    "\n",
    "Accuracy does not perform well with imbalanced data sets. For example, if you have 95 negative and 5 positive samples, classifying all as negative gives 0.95 accuracy score. Therefore, often precision or recall is used to rank models.\n",
    "\n",
    "#### F-beta score\n",
    "\n",
    "We can combine Precision and Recall to a F-$\\beta$ score to get a better evaluation metric for our model. \n",
    "\n",
    "$$ F\\beta = (1 + \\beta^2) \\frac{Precision * Recall}{(\\beta^2 * Precision) + Recall} $$\n",
    "\n",
    "When $\\beta$ is equal to 1, the F-$\\beta$ score is also known as an F1 score, which gives the harmonic average of of Accuracy and Recall. When $\\beta$ is equal to 0, we get the accuracy and when it is infinity we get the Recall.\n",
    "\n",
    "#### Receiver Operating Characteristic (ROC) curve and Area Under the Curve\n",
    "\n",
    "By finding different thresholds for our classification metrics, we can measure the area under the curve (where the curve is known as a ROC curve). Similar to each of the other metrics above, when the AUC is higher (closer to 1), this suggests that our model performance is better than when our metric is close to 0.\n",
    "\n",
    "![Illustration for ROC and AOC](./images/roc_aoc.png \"Illustration for ROC and AOC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "You want to measure how well your algorithms are performing on predicting numeric values? In these cases, there are three main metrics that are frequently used. mean absolute error, mean squared error, and r2 values.\n",
    "\n",
    "As an important note, optimizing on the mean absolute error may lead to a different 'best model' than if you optimize on the mean squared error. However, optimizing on the mean squared error will always lead to the same 'best' model as if you were to optimize on the r2 value. Again, if you choose a model with the best r2 value (the highest), it will also be the model that has the lowest (MSE).\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "The first metric you saw was the mean absolute error. This is a useful metric to optimize on when the value you are trying to predict follows a skewed distribution. Optimizing on an absolute value is particularly helpful in these cases because outliers will not influence models attempting to optimize on this metric as much as if you use the mean squared error. The optimal value for this technique is the median value. When you optimize for the R2 value of the mean squared error, the optimal value is actually the mean.\n",
    "\n",
    "![Illustration for MAE](./images/mae.png \"Illustration for MAE\")\n",
    "\n",
    "#### Mean Square Error (MSE)\n",
    "\n",
    "The mean squared error is by far the most used metric for optimization in regression problems. Similar to with MAE, you want to find a model that minimizes this value. This metric can be greatly impacted by skewed distributions and outliers. When a model is considered optimal via MAE, but not for MSE, it is useful to keep this in mind. In many cases, it is easier to actually optimize on MSE, as the a quadratic term is differentiable. However, an absolute value is not differentiable. This factor makes this metric better for gradient based optimization algorithms.\n",
    "\n",
    "![Illustration for MSE](./images/mse.png \"Illustration for MSE\")\n",
    "\n",
    "#### R2 Score\n",
    "\n",
    "Finally, the r2 value is another common metric when looking at regression values. Optimizing a model to have the lowest MSE will also optimize a model to have the the highest R2 value. This is a convenient feature of this metric. The R2 value is frequently interpreted as the 'amount of variability' captured by a model. Therefore, you can think of MSE, as the average amount you miss by across all the points, and the R2 value as the amount of the variability in the points that you capture with a model.\n",
    "\n",
    "![Illustration for R2 Score](./images/r2_score.png \"R2 Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
